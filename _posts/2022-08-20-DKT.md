```python
%pwd
```




    'C:\\Users\\53789\\Deep Learning\\DKT'




```python
import os

os.getcwd()
```




    'C:\\Users\\53789\\Deep Learning\\DKT'




```python
!nvidia-smi
```

    Sat Aug 20 18:18:54 2022       
    +-----------------------------------------------------------------------------+
    | NVIDIA-SMI 516.94       Driver Version: 516.94       CUDA Version: 11.7     |
    |-------------------------------+----------------------+----------------------+
    | GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |
    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
    |                               |                      |               MIG M. |
    |===============================+======================+======================|
    |   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |
    |  0%   55C    P8    17W / 200W |   3857MiB /  8192MiB |      0%      Default |
    |                               |                      |                  N/A |
    +-------------------------------+----------------------+----------------------+
                                                                                   
    +-----------------------------------------------------------------------------+
    | Processes:                                                                  |
    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
    |        ID   ID                                                   Usage      |
    |=============================================================================|
    |    0   N/A  N/A      2392    C+G   ...lPanel\SystemSettings.exe    N/A      |
    |    0   N/A  N/A      4332    C+G   C:\Windows\explorer.exe         N/A      |
    |    0   N/A  N/A      4792    C+G   ...2txyewy\TextInputHost.exe    N/A      |
    |    0   N/A  N/A      7456    C+G   ...5n1h2txyewy\SearchApp.exe    N/A      |
    |    0   N/A  N/A      9412    C+G   ...\app-1.0.9006\Discord.exe    N/A      |
    |    0   N/A  N/A      9548    C+G   ...ekyb3d8bbwe\YourPhone.exe    N/A      |
    |    0   N/A  N/A     13168    C+G   ...ge\Application\msedge.exe    N/A      |
    |    0   N/A  N/A     13532      C   ...3789\anaconda3\python.exe    N/A      |
    +-----------------------------------------------------------------------------+
    


```python
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["CUDA_VISIBLE_DEVICES"] = "0"
import numpy as np
import pandas as pd
import torch
from torch.utils.tensorboard import SummaryWriter
```


```python
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")
```

    Using cuda device
    


```python
Dataset_dir = 'Data/skill_builder_data_2015.csv'
```


```python
def match_seq_len(q_seqs, r_seqs, seq_len, pad_val=-1):
    '''
        Args:
            q_seqs: the question(KC) sequences with the size of \
                [batch_size, some_sequence_length]
            r_seqs: the response sequences with the size of \
                [batch_size, some_sequence_length]

            Note that the "some_sequence_length" is not uniform over \
                the whole batch of q_seqs and r_seqs

            seq_len: the sequence length to match the q_seqs, r_seqs \
                to same length
            pad_val: the padding value for the sequence with the length \
                longer than seq_len

        Returns:
            proc_q_seqs: the processed q_seqs with the size of \
                [batch_size, seq_len + 1]
            proc_r_seqs: the processed r_seqs with the size of \
                [batch_size, seq_len + 1]
    '''
    proc_q_seqs = []
    proc_r_seqs = []

    for q_seq, r_seq in zip(q_seqs, r_seqs):
        i = 0
        while i + seq_len + 1 < len(q_seq):
            proc_q_seqs.append(q_seq[i:i + seq_len + 1])
            proc_r_seqs.append(r_seq[i:i + seq_len + 1])

            i += seq_len + 1

        proc_q_seqs.append(
            np.concatenate(
                [
                    q_seq[i:],
                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))
                ]
            )
        )
        proc_r_seqs.append(
            np.concatenate(
                [
                    r_seq[i:],
                    np.array([pad_val] * (i + seq_len + 1 - len(q_seq)))
                ]
            )
        )

    return proc_q_seqs, proc_r_seqs
```


```python
from torch.utils.data import Dataset

class assist2015(Dataset):
    def __init__(self, data_dir=Dataset_dir):
        super().__init__()

        self.data_dir = data_dir
        self.q_seqs, self.r_seqs, self.num_s, self.num_q = self.preprocess()
        self.q_seqs, self.r_seqs = match_seq_len(self.q_seqs, self.r_seqs, 100)
        self.len = len(self.q_seqs)

    def __getitem__(self, idx):
        return self.q_seqs[idx], self.r_seqs[idx]

    def __len__(self):
        return self.len

    def preprocess(self):
        df=pd.read_csv(self.data_dir)
        df= df[(df["correct"] == 0).values + (df["correct"] == 1).values] # 0과 1이 아닌 다른 수 제거

        student_list = np.unique(df['user_id'].values)
        question_list = np.unique(df['sequence_id'].values) # len(question_list) --> 100 questions
        num_s = len(student_list) # 학생 수
        num_q = len(question_list) # 문제 수

        s2idx = {u: idx for idx, u in enumerate(student_list)}
        q2idx = {u: idx for idx, u in enumerate(question_list)}

        q_seqs = []
        r_seqs = []

        for student in student_list:
            df_student = df[df['user_id'] == student].sort_values('log_id')

            q_seq = np.array([q2idx[q] for q in df_student['sequence_id'].values]) # 문제 넘버를 0부터 시작하게함
            r_seq = df_student['correct'].values

            q_seqs.append(q_seq)
            r_seqs.append(r_seq)
        
        return q_seqs, r_seqs, num_s, num_q

```


```python
dataset = assist2015(Dataset_dir)
dataset
```




    <__main__.assist2015 at 0x29d14f627f0>




```python
len(dataset)
```




    21377




```python
from torch.nn.utils.rnn import pad_sequence

if torch.cuda.is_available():
    from torch.cuda import FloatTensor
    torch.set_default_tensor_type(torch.cuda.FloatTensor)
else:
    from torch import FloatTensor

def mycollate_fn(batch, pad_val=-1):
    q_seqs = []
    r_seqs = []
    q_shift = []
    r_shift = []

    for q_seq, r_seq in batch:
        q_seqs.append(FloatTensor(q_seq[:-1])) # 마지막 문제 제거
        r_seqs.append(FloatTensor(r_seq[:-1]))
        q_shift.append(FloatTensor(q_seq[1:])) # 첫 문제 제거
        r_shift.append(FloatTensor(r_seq[1:])) 

    q_seqs = pad_sequence(q_seqs, batch_first=True, padding_value=pad_val) # 각 배치별 최대 길이로 padding
    r_seqs = pad_sequence(r_seqs, batch_first=True, padding_value=pad_val)
    q_shift = pad_sequence(q_shift, batch_first=True, padding_value=pad_val)
    r_shift = pad_sequence(r_shift, batch_first=True, padding_value=pad_val)

    mask_seqs = (q_seqs != pad_val) * (q_shift != pad_val) # padding파트 날리기

    q_seqs = q_seqs * mask_seqs
    r_seqs = r_seqs * mask_seqs
    q_shift = q_shift * mask_seqs
    r_shift = r_shift * mask_seqs

    return q_seqs, r_seqs, q_shift, r_shift, mask_seqs
```


```python
from torch.nn import Module, Embedding, LSTM, Linear, Dropout, BCELoss
from torch.nn.functional import one_hot
from sklearn import metrics

class DKT(Module):
    def __init__(self, num_q, emb_size, hidden_size):
        super().__init__()
        self.num_q = num_q
        self.emb_size = emb_size
        self.hidden_size = hidden_size

        self.embedding = Embedding(self.num_q * 2, self.emb_size) # 정오답을 동시에 표현하기 위해 2배를 해줌
        self.lstm = LSTM(self.emb_size, self.hidden_size, batch_first=True)
        self.fc = Linear(self.hidden_size, self.num_q)
        self.dropout = Dropout()

    def forward(self, question, respond):
        input = question + self.num_q * respond # 정오답을 동시에 표현하기 위한 방법(해당 문제를 틀렸으면 문제번호 그대로, 맞췄으면 문제 수만큼 더한 번호)
        
        embedded = self.embedding(input)
        hidden, _ = self.lstm(embedded)
        x = self.fc(hidden)
        x = self.dropout(x)
        output = torch.sigmoid(x)
        
        return output
```

    C:\Users\53789\anaconda3\lib\site-packages\scipy\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1
      warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
    


```python
from torch.utils.data import DataLoader, random_split

if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

config = {
    "train_config": {
        "batch_size": 256,
        "num_epochs": 100,
        "train_ratio": 0.9,
        "learning_rate": 0.001,
    },
    "model_config":{
        'emb_size': 100,
        "hidden_size": 100
    }
}

batch_size = config['train_config']["batch_size"]
num_epochs = config['train_config']["num_epochs"]
train_ratio = config['train_config']["train_ratio"]
learning_rate = config['train_config']["learning_rate"]

model_config = config["model_config"]

writer = SummaryWriter()

dataset = assist2015(Dataset_dir)

model = DKT(dataset.num_q, **model_config).to(device)

train_size = int(len(dataset) * train_ratio)
test_size = len(dataset) - train_size

train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator(device=device))

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=mycollate_fn, generator=torch.Generator(device=device))
test_loader = DataLoader(test_dataset, batch_size=test_size, shuffle=False, collate_fn=mycollate_fn, generator=torch.Generator(device=device))

# optimizer = torch.optim.Adam(model.parameters(), learning_rate, capturable=True)
optimizer = torch.optim.SGD(model.parameters(), learning_rate, momentum=0.9)
custom_loss = BCELoss()
```


```python
batch_size
```




    256




```python
for i in iter(train_loader):
    print(i, i[0].shape)
    break
```

    (tensor([[64., 64., 64.,  ..., 86., 86., 86.],
            [49., 36., 36.,  ..., 75., 75., 75.],
            [59., 59., 59.,  ..., -0., -0., -0.],
            ...,
            [63., 63., 63.,  ..., -0., -0., -0.],
            [97., 97., 97.,  ..., -0., -0., -0.],
            [ 6.,  6.,  6.,  ..., -0., -0., -0.]]), tensor([[0., 1., 1.,  ..., 0., 1., 0.],
            [1., 1., 1.,  ..., 1., 1., 0.],
            [0., 0., 1.,  ..., -0., -0., -0.],
            ...,
            [0., 0., 0.,  ..., -0., -0., -0.],
            [1., 1., 1.,  ..., -0., -0., -0.],
            [1., 1., 1.,  ..., -0., -0., -0.]]), tensor([[64., 64., 64.,  ..., 86., 86., 86.],
            [36., 36., 36.,  ..., 75., 75., 75.],
            [59., 59., 59.,  ..., -0., -0., -0.],
            ...,
            [63., 63., 63.,  ..., -0., -0., -0.],
            [97., 97., 96.,  ..., -0., -0., -0.],
            [ 6.,  6.,  7.,  ..., -0., -0., -0.]]), tensor([[1., 1., 1.,  ..., 1., 0., 1.],
            [1., 1., 1.,  ..., 1., 0., 1.],
            [0., 1., 1.,  ..., -0., -0., -0.],
            ...,
            [0., 0., 1.,  ..., -0., -0., -0.],
            [1., 1., 1.,  ..., -0., -0., -0.],
            [1., 1., 1.,  ..., -0., -0., -0.]]), tensor([[ True,  True,  True,  ...,  True,  True,  True],
            [ True,  True,  True,  ...,  True,  True,  True],
            [ True,  True,  True,  ..., False, False, False],
            ...,
            [ True,  True,  True,  ..., False, False, False],
            [ True,  True,  True,  ..., False, False, False],
            [ True,  True,  True,  ..., False, False, False]])) torch.Size([256, 100])
    


```python
print(test_size)
print(train_size)
print(len(dataset))
```

    2138
    19239
    21377
    


```python
import torch, gc
gc.collect()
torch.cuda.empty_cache()
```


```python
%load_ext tensorboard
# %tensorboard --logdir runs/
%tensorboard --logdir=runs/ --port=8080
from tqdm import tqdm

train_step=0
loss_means = []
for i in range(1, num_epochs +1):

    # validation step
    with torch.no_grad():
        val_loss = 0.0
        val_auc = 0.0

        for val_batch_idx, val_batch in enumerate(tqdm(test_loader, desc="validation")):
            problem_seqs, answer_seqs, problem_shift_seqs, answer_shift_seqs, mask_seqs = val_batch

            # forward
            output = model(problem_seqs.long(), answer_seqs.long()) # [batch_size, max_seq_length, num_problems]
            output = (output * one_hot(problem_shift_seqs.long(), dataset.num_q)).sum(-1) # [batch_size, max_seq_length]

            output = torch.masked_select(output, mask_seqs).detach().cpu() # [batch_size, mask_seq_length]
            target = torch.masked_select(answer_shift_seqs, mask_seqs).detach().cpu() # [batch_size, mask_seq_length]

            # loss & auc
            val_loss += custom_loss(output, target)

            val_auc = metrics.roc_auc_score(y_true=target.numpy(), y_score=output.numpy())

    # valid step logging
    val_epoch_loss = val_loss / len(test_loader)
    val_epoch_auc = val_auc / len(test_loader)

    print("Epoch: {}, val loss: {}".format(i, val_epoch_loss))
    print("Epoch: {}, val auc: {}".format(i, val_epoch_auc))
    writer.add_scalar('Loss/val', val_epoch_loss, train_step)
    writer.add_scalar('AUC/val', val_epoch_auc, train_step)

    # train step
    loss_mean = []
    current_loss = 0.0
    for batch_idx, batch in enumerate(tqdm(train_loader, desc="training")):
        

        problem_seqs, answer_seqs, problem_shift_seqs, answer_shift_seqs, mask_seqs = batch
        problem_seqs, answer_seqs = problem_seqs.to(device), answer_seqs.to(device)

        output = model(problem_seqs.long(), answer_seqs.long())
        output = (output * one_hot(problem_shift_seqs.long(), dataset.num_q)).sum(-1)


        output = torch.masked_select(output, mask_seqs)
        target = torch.masked_select(answer_shift_seqs, mask_seqs)


        loss = custom_loss(output, target)

        optimizer.zero_grad()
        loss.backward()
        # for param in model.parameters():
        #     print(param.grad.data.sum())

        optimizer.step()

        current_loss += loss.detach().cpu().numpy()

        if train_step % 100 == 0:
            train_loss = current_loss / 100

            print("{} >> trian_loss: {}".format(train_step, train_loss))
            writer.add_scalar('Loss/train', train_loss, train_step)
            current_loss = 0
        
    # for name, child in model.named_children():
    #     for param in child.parameters():
    #         print(name, param)

        train_step += 1
```

    The tensorboard extension is already loaded. To reload it, use:
      %reload_ext tensorboard
    


    Reusing TensorBoard on port 8080 (pid 4504), started 0:17:36 ago. (Use '!kill 4504' to kill it.)




<iframe id="tensorboard-frame-e81d7fa91af6a054" width="100%" height="800" frameborder="0">
</iframe>
<script>
  (function() {
    const frame = document.getElementById("tensorboard-frame-e81d7fa91af6a054");
    const url = new URL("/", window.location);
    const port = 8080;
    if (port) {
      url.port = port;
    }
    frame.src = url;
  })();
</script>



    validation: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]
    

    Epoch: 1, val loss: 0.6171543598175049
    Epoch: 1, val auc: 0.5645683645849641
    

    training:   3%|▎         | 2/76 [00:00<00:06, 12.20it/s]

    0 >> trian_loss: 0.006131138801574707
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.37it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]
    

    Epoch: 2, val loss: 0.6169902086257935
    Epoch: 2, val auc: 0.5646133843202613
    

    training:  34%|███▍      | 26/76 [00:01<00:03, 12.72it/s]

    100 >> trian_loss: 0.1539218533039093
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.82it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s]
    

    Epoch: 3, val loss: 0.6164844632148743
    Epoch: 3, val auc: 0.5650069829984405
    

    training:  67%|██████▋   | 51/76 [00:04<00:02, 11.33it/s]

    200 >> trian_loss: 0.30204513370990754
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.56it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
    

    Epoch: 4, val loss: 0.6167861223220825
    Epoch: 4, val auc: 0.565662099531343
    

    training:  97%|█████████▋| 74/76 [00:06<00:00,  9.91it/s]

    300 >> trian_loss: 0.4495640164613724
    

    training: 100%|██████████| 76/76 [00:06<00:00, 10.97it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  2.25it/s]
    

    Epoch: 5, val loss: 0.6158590316772461
    Epoch: 5, val auc: 0.5666071311637052
    

    training: 100%|██████████| 76/76 [00:05<00:00, 15.02it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
    

    Epoch: 6, val loss: 0.6157793998718262
    Epoch: 6, val auc: 0.567711338961255
    

    training:  29%|██▉       | 22/76 [00:01<00:03, 13.81it/s]

    400 >> trian_loss: 0.12968483865261077
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.69it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]
    

    Epoch: 7, val loss: 0.6162925958633423
    Epoch: 7, val auc: 0.5665273363878386
    

    training:  59%|█████▉    | 45/76 [00:03<00:02, 12.49it/s]

    500 >> trian_loss: 0.2772563821077347
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.25it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]
    

    Epoch: 8, val loss: 0.6170094609260559
    Epoch: 8, val auc: 0.5650169313250047
    

    training:  92%|█████████▏| 70/76 [00:06<00:00,  9.73it/s]

    600 >> trian_loss: 0.4249153310060501
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.35it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]
    

    Epoch: 9, val loss: 0.6175664067268372
    Epoch: 9, val auc: 0.5632007748710282
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.39it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]
    

    Epoch: 10, val loss: 0.6153881549835205
    Epoch: 10, val auc: 0.5678675467405522
    

    training:  24%|██▎       | 18/76 [00:01<00:04, 14.19it/s]

    700 >> trian_loss: 0.10448035538196564
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.53it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]
    

    Epoch: 11, val loss: 0.6165518164634705
    Epoch: 11, val auc: 0.5651103602124516
    

    training:  55%|█████▌    | 42/76 [00:03<00:03, 11.05it/s]

    800 >> trian_loss: 0.2522208631038666
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.38it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]
    

    Epoch: 12, val loss: 0.6151953935623169
    Epoch: 12, val auc: 0.5679004759484534
    

    training:  87%|████████▋ | 66/76 [00:05<00:00, 12.80it/s]

    900 >> trian_loss: 0.4003270709514618
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.58it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s]
    

    Epoch: 13, val loss: 0.6158273220062256
    Epoch: 13, val auc: 0.5655885684730887
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.35it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]
    

    Epoch: 14, val loss: 0.6142346858978271
    Epoch: 14, val auc: 0.5686322071797869
    

    training:  18%|█▊        | 14/76 [00:01<00:05, 11.92it/s]

    1000 >> trian_loss: 0.07983684122562408
    

    training: 100%|██████████| 76/76 [00:05<00:00, 12.71it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s]
    

    Epoch: 15, val loss: 0.616338849067688
    Epoch: 15, val auc: 0.5648440446040672
    

    training:  50%|█████     | 38/76 [00:03<00:02, 12.81it/s]

    1100 >> trian_loss: 0.22742920994758606
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.86it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s]
    

    Epoch: 16, val loss: 0.6166726350784302
    Epoch: 16, val auc: 0.5644889667307449
    

    training:  83%|████████▎ | 63/76 [00:05<00:01, 12.69it/s]

    1200 >> trian_loss: 0.37550938844680787
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.12it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]
    

    Epoch: 17, val loss: 0.6155011653900146
    Epoch: 17, val auc: 0.5669264472786442
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.89it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]
    

    Epoch: 18, val loss: 0.6153782606124878
    Epoch: 18, val auc: 0.5665373082356855
    

    training:  13%|█▎        | 10/76 [00:00<00:04, 13.46it/s]

    1300 >> trian_loss: 0.05539826095104217
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.81it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]
    

    Epoch: 19, val loss: 0.6151555776596069
    Epoch: 19, val auc: 0.5673133112255228
    

    training:  45%|████▍     | 34/76 [00:03<00:04,  9.51it/s]

    1400 >> trian_loss: 0.203022021651268
    

    training: 100%|██████████| 76/76 [00:07<00:00,  9.97it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
    

    Epoch: 20, val loss: 0.6147059202194214
    Epoch: 20, val auc: 0.5683002513154983
    

    training:  78%|███████▊  | 59/76 [00:04<00:01, 12.46it/s]

    1500 >> trian_loss: 0.35040394723415375
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.45it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s]
    

    Epoch: 21, val loss: 0.6154228448867798
    Epoch: 21, val auc: 0.567248033785406
    

    training: 100%|██████████| 76/76 [00:06<00:00, 10.92it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]
    

    Epoch: 22, val loss: 0.6140649914741516
    Epoch: 22, val auc: 0.5694579535665416
    

    training:   8%|▊         | 6/76 [00:00<00:06, 11.44it/s]

    1600 >> trian_loss: 0.03069178283214569
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.57it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s]
    

    Epoch: 23, val loss: 0.6150326728820801
    Epoch: 23, val auc: 0.5673420307118331
    

    training:  39%|███▉      | 30/76 [00:02<00:03, 12.14it/s]

    1700 >> trian_loss: 0.1784754627943039
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.13it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s]
    

    Epoch: 24, val loss: 0.6151024699211121
    Epoch: 24, val auc: 0.5676818115188803
    

    training:  74%|███████▎  | 56/76 [00:04<00:01, 12.50it/s]

    1800 >> trian_loss: 0.3258377683162689
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.08it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s]
    

    Epoch: 25, val loss: 0.6137416362762451
    Epoch: 25, val auc: 0.5697332025581372
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.64it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]
    

    Epoch: 26, val loss: 0.6156286001205444
    Epoch: 26, val auc: 0.5667478901644534
    

    training:   1%|▏         | 1/76 [00:00<00:08,  8.65it/s]

    1900 >> trian_loss: 0.006076173186302185
    

    training: 100%|██████████| 76/76 [00:07<00:00,  9.61it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.44it/s]
    

    Epoch: 27, val loss: 0.613860547542572
    Epoch: 27, val auc: 0.5703513577472332
    

    training:  36%|███▌      | 27/76 [00:01<00:03, 14.84it/s]

    2000 >> trian_loss: 0.1540974485874176
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.53it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s]
    

    Epoch: 28, val loss: 0.6159360408782959
    Epoch: 28, val auc: 0.5649952488184788
    

    training:  66%|██████▌   | 50/76 [00:04<00:02,  9.76it/s]

    2100 >> trian_loss: 0.3010366815328598
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.08it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
    

    Epoch: 29, val loss: 0.6139737367630005
    Epoch: 29, val auc: 0.5694979503318948
    

    training:  97%|█████████▋| 74/76 [00:06<00:00,  9.51it/s]

    2200 >> trian_loss: 0.44883002161979674
    

    training: 100%|██████████| 76/76 [00:07<00:00, 10.79it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]
    

    Epoch: 30, val loss: 0.615086555480957
    Epoch: 30, val auc: 0.5689482732414737
    

    training: 100%|██████████| 76/76 [00:07<00:00, 10.38it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s]
    

    Epoch: 31, val loss: 0.6128924489021301
    Epoch: 31, val auc: 0.5709329014716042
    

    training:  29%|██▉       | 22/76 [00:01<00:03, 14.25it/s]

    2300 >> trian_loss: 0.1292309981584549
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.61it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]
    

    Epoch: 32, val loss: 0.612806499004364
    Epoch: 32, val auc: 0.5722970535500578
    

    training:  61%|██████    | 46/76 [00:04<00:03,  8.62it/s]

    2400 >> trian_loss: 0.2767259329557419
    

    training: 100%|██████████| 76/76 [00:06<00:00, 10.93it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]
    

    Epoch: 33, val loss: 0.6142221093177795
    Epoch: 33, val auc: 0.569948775703117
    

    training:  92%|█████████▏| 70/76 [00:05<00:00, 10.65it/s]

    2500 >> trian_loss: 0.42420789480209353
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.20it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]
    

    Epoch: 34, val loss: 0.6159865260124207
    Epoch: 34, val auc: 0.5654184790207093
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.15it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]
    

    Epoch: 35, val loss: 0.6151504516601562
    Epoch: 35, val auc: 0.568048958162412
    

    training:  26%|██▋       | 20/76 [00:01<00:03, 16.71it/s]

    2600 >> trian_loss: 0.1046991890668869
    

    training: 100%|██████████| 76/76 [00:05<00:00, 14.04it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s]
    

    Epoch: 36, val loss: 0.6144174337387085
    Epoch: 36, val auc: 0.5687109658431457
    

    training:  55%|█████▌    | 42/76 [00:03<00:03,  9.79it/s]

    2700 >> trian_loss: 0.2517139369249344
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.17it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]
    

    Epoch: 37, val loss: 0.6153715252876282
    Epoch: 37, val auc: 0.5668097323212157
    

    training:  87%|████████▋ | 66/76 [00:06<00:00, 12.14it/s]

    2800 >> trian_loss: 0.399224237203598
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.01it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]
    

    Epoch: 38, val loss: 0.6130632758140564
    Epoch: 38, val auc: 0.5717111195782503
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.29it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]
    

    Epoch: 39, val loss: 0.6137076020240784
    Epoch: 39, val auc: 0.5677177061725115
    

    training:  18%|█▊        | 14/76 [00:01<00:04, 13.02it/s]

    2900 >> trian_loss: 0.07991282284259796
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.34it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]
    

    Epoch: 40, val loss: 0.6146484613418579
    Epoch: 40, val auc: 0.5684182540631606
    

    training:  50%|█████     | 38/76 [00:03<00:02, 12.73it/s]

    3000 >> trian_loss: 0.22730509340763091
    

    training: 100%|██████████| 76/76 [00:07<00:00, 10.74it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]
    

    Epoch: 41, val loss: 0.6174314618110657
    Epoch: 41, val auc: 0.5624466531425563
    

    training:  82%|████████▏ | 62/76 [00:05<00:01,  9.23it/s]

    3100 >> trian_loss: 0.374542133808136
    

    training: 100%|██████████| 76/76 [00:07<00:00, 10.46it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s]
    

    Epoch: 42, val loss: 0.6138067245483398
    Epoch: 42, val auc: 0.5711138148303005
    

    training: 100%|██████████| 76/76 [00:07<00:00, 10.54it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]
    

    Epoch: 43, val loss: 0.6139844655990601
    Epoch: 43, val auc: 0.5696173231942805
    

    training:  16%|█▌        | 12/76 [00:00<00:04, 14.81it/s]

    3200 >> trian_loss: 0.05537907183170319
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.97it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s]
    

    Epoch: 44, val loss: 0.6143081784248352
    Epoch: 44, val auc: 0.568833364130533
    

    training:  47%|████▋     | 36/76 [00:03<00:02, 14.71it/s]

    3300 >> trian_loss: 0.20269410967826842
    

    training: 100%|██████████| 76/76 [00:05<00:00, 12.86it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s]
    

    Epoch: 45, val loss: 0.6147474050521851
    Epoch: 45, val auc: 0.5680167769313044
    

    training:  76%|███████▋  | 58/76 [00:04<00:01, 11.54it/s]

    3400 >> trian_loss: 0.34981958866119384
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.35it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]
    

    Epoch: 46, val loss: 0.6161889433860779
    Epoch: 46, val auc: 0.5653751792792177
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.23it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s]
    

    Epoch: 47, val loss: 0.6145125031471252
    Epoch: 47, val auc: 0.5688228748144643
    

    training:   8%|▊         | 6/76 [00:00<00:06, 11.32it/s]

    3500 >> trian_loss: 0.030729169249534605
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.58it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s]
    

    Epoch: 48, val loss: 0.613964319229126
    Epoch: 48, val auc: 0.5690049560048517
    

    training:  39%|███▉      | 30/76 [00:02<00:04, 10.92it/s]

    3600 >> trian_loss: 0.1780971086025238
    

    training: 100%|██████████| 76/76 [00:05<00:00, 13.71it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]
    

    Epoch: 49, val loss: 0.6148111820220947
    Epoch: 49, val auc: 0.5676486776758188
    

    training:  72%|███████▏  | 55/76 [00:04<00:01, 11.93it/s]

    3700 >> trian_loss: 0.32558948814868927
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.29it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s]
    

    Epoch: 50, val loss: 0.6147603392601013
    Epoch: 50, val auc: 0.5689792913451465
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.42it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s]
    

    Epoch: 51, val loss: 0.613172709941864
    Epoch: 51, val auc: 0.5718304689193532
    

    training:   3%|▎         | 2/76 [00:00<00:05, 14.39it/s]

    3800 >> trian_loss: 0.0059576076269149784
    

    training: 100%|██████████| 76/76 [00:06<00:00, 12.35it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s]
    

    Epoch: 52, val loss: 0.6138561367988586
    Epoch: 52, val auc: 0.5705462027617373
    

    training:  34%|███▍      | 26/76 [00:01<00:03, 14.80it/s]

    3900 >> trian_loss: 0.15355503678321838
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.84it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]
    

    Epoch: 53, val loss: 0.6147328615188599
    Epoch: 53, val auc: 0.5690345416624015
    

    training:  66%|██████▌   | 50/76 [00:04<00:02, 12.31it/s]

    4000 >> trian_loss: 0.3010453099012375
    

    training: 100%|██████████| 76/76 [00:06<00:00, 11.87it/s]
    validation: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s]
    

    Epoch: 54, val loss: 0.6138097047805786
    Epoch: 54, val auc: 0.5692355910032787
    

    training:  58%|█████▊    | 44/76 [00:03<00:02, 12.46it/s]
    


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    Input In [25], in <cell line: 8>()
         40 loss_mean = []
         41 current_loss = 0.0
    ---> 42 for batch_idx, batch in enumerate(tqdm(train_loader, desc="training")):
         45     problem_seqs, answer_seqs, problem_shift_seqs, answer_shift_seqs, mask_seqs = batch
         46     problem_seqs, answer_seqs = problem_seqs.to(device), answer_seqs.to(device)
    

    File ~\anaconda3\lib\site-packages\tqdm\std.py:1195, in tqdm.__iter__(self)
       1192 time = self._time
       1194 try:
    -> 1195     for obj in iterable:
       1196         yield obj
       1197         # Update and possibly print the progressbar.
       1198         # Note: does not call self.update(1) for speed optimisation.
    

    File ~\anaconda3\lib\site-packages\torch\utils\data\dataloader.py:681, in _BaseDataLoaderIter.__next__(self)
        678 if self._sampler_iter is None:
        679     # TODO(https://github.com/pytorch/pytorch/issues/76750)
        680     self._reset()  # type: ignore[call-arg]
    --> 681 data = self._next_data()
        682 self._num_yielded += 1
        683 if self._dataset_kind == _DatasetKind.Iterable and \
        684         self._IterableDataset_len_called is not None and \
        685         self._num_yielded > self._IterableDataset_len_called:
    

    File ~\anaconda3\lib\site-packages\torch\utils\data\dataloader.py:721, in _SingleProcessDataLoaderIter._next_data(self)
        719 def _next_data(self):
        720     index = self._next_index()  # may raise StopIteration
    --> 721     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
        722     if self._pin_memory:
        723         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)
    

    File ~\anaconda3\lib\site-packages\torch\utils\data\_utils\fetch.py:52, in _MapDatasetFetcher.fetch(self, possibly_batched_index)
         50 else:
         51     data = self.dataset[possibly_batched_index]
    ---> 52 return self.collate_fn(data)
    

    Input In [9], in mycollate_fn(batch, pad_val)
         22 r_seqs = pad_sequence(r_seqs, batch_first=True, padding_value=pad_val)
         23 q_shift = pad_sequence(q_shift, batch_first=True, padding_value=pad_val)
    ---> 24 r_shift = pad_sequence(r_shift, batch_first=True, padding_value=pad_val)
         26 mask_seqs = (q_seqs != pad_val) * (q_shift != pad_val) # padding파트 날리기
         28 q_seqs = q_seqs * mask_seqs
    

    File ~\anaconda3\lib\site-packages\torch\nn\utils\rnn.py:396, in pad_sequence(sequences, batch_first, padding_value)
        392         sequences = sequences.unbind(0)
        394 # assuming trailing dimensions and type of all the Tensors
        395 # in sequences are same and fetching those from sequences[0]
    --> 396 return torch._C._nn.pad_sequence(sequences, batch_first, padding_value)
    

    KeyboardInterrupt: 



```python

```
